  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2021
  img: coseco.png
  title: "CoSe-Co: Text Conditioned Generative CommonSense Contextualizer for Language Models"
  authors: "<u>Rachit Bansal</u>, Milan Aggarwal, Sumit Bhatia, Jivat Neet Kaur, Balaji Krishnamurthy"
  conf_name: CSKB AKBC
  conf_year: 2021
  url: "https://openreview.net/pdf?id=1yieqYLUIXj"
  abstract: Pre-trained Language Models (PTLMs) have been shown to perform well on natural language reasoning tasks requiring commonsense. Prior work has leveraged structured commonsense present in knowledge graphs (KGs) to assist PTLMs. Some of these methods use KGs as separate static modules which limits knowledge coverage since KGs are finite, sparse, and noisy. Other methods have attempted to obtain generalized and scalable commonsense by training PTLMs on KGs. Since they are trained on symbolic KG phrases, applying them on natural language text during inference leads to input distribution shift. To this end, we propose a task agnostic sentence-conditioned generative CommonSense Contextualizer (CoSe-Co), which is trained to generate contextually relevant commonsense inferences given a natural language input. We devise a method to create semantically related sentence-commonsense pairs to train CoSe-Co. We observe commonsense inferences generated by CoSe-Co contain novel concepts that are relevant to the entire sentence context. We evaluate CoSe-Co on multi-choice QA and open-ended commonsense reasoning tasks on the CSQA, ARC, QASC, and OBQA datasets. CoSe-Co outperforms state-of-the-art methods in both these settings, while being task-agnostic, and performs especially well in low data regimes showing it is more robust and generalises better. 
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2021
  img: no_need_to_know_everything.png
  title: "No Need to Know Everything! Efficiently Augmenting Language Models With External Knowledge "
  authors: "Jivat Neet Kaur, Sumit Bhatia, Milan Aggarwal, <u>Rachit Bansal</u>, Balaji Krishnamurthy"
  conf_name: CSKB AKBC
  conf_year: 2021
  url: "https://openreview.net/pdf?id=fn5K7VfI3MV"
  abstract: Large transformer-based pre-trained language models have achieved impressive performance on a variety of knowledge-intensive tasks and can capture semantic, syntactic, and factual knowledge in their parameters. However, storing large amounts of factual knowledge in the parameters of the model is sub-optimal given the resource requirements and ever-growing amounts of knowledge. Instead of packing all the knowledge in the model parameters, we argue that a more efficient alternative is to provide contextually relevant structured knowledge to the model and train it to use that knowledge. This allows the training of the language model to be de-coupled from the external knowledge source and the latter can be updated without affecting the parameters of the language model. Empirical evaluation using different subsets of LAMA probe reveals that such an approach allows smaller language models with access to external knowledge to achieve significant and robust outperformance over much larger language models.
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2021
  img: how_low_too_low.png
  title: "How Low is Too Low? A Computational Perspective on Extremely Low-Resource Languages"
  authors: "<u>Rachit Bansal</u>, Himanshu Choudhary, Ravneet Punia, Niko Schenk, Jacob L Dahl, Émilie Pagé-Perron"
  conf_name: ACL SRW
  conf_year: 2021
  url: "https://arxiv.org/pdf/2105.14515.pdf"
  code: "https://github.com/cdli-gh/Semi-Supervised-NMT-for-Sumerian-English"
  abstract: "Despite the recent advancements of attention-based deep learning architectures across a majority of Natural Language Processing tasks, their application remains limited in a low-resource setting because of a lack of pre-trained models for such languages. In this study, we make the first attempt to investigate the challenges of adapting these techniques for an extremely low-resource language -- Sumerian cuneiform -- one of the world's oldest written languages attested from at least the beginning of the 3rd millennium BC. Specifically, we introduce the first cross-lingual information extraction pipeline for Sumerian, which includes part-of-speech tagging, named entity recognition, and machine translation. We further curate InterpretLR, an interpretability toolkit for low-resource NLP, and use it alongside human attributions to make sense of the models. We emphasize on human evaluations to gauge all our techniques. Notably, most components of our pipeline can be generalised to any other language to obtain an interpretable execution of the techniques, especially in a low-resource setting. We publicly release all software, model checkpoints, and a novel dataset with domain-specific pre-processing to promote further research."
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2021
  img: endemic.png
  title: "Combining exogenous and endogenous signals with a semi-supervised co-attention network for early detection of COVID-19 fake tweets"
  authors: "<u>Rachit Bansal</u>, William Scott Paka, Shubhashis Sengupta, Tanmoy Chakraborty"
  conf_name: PAKDD
  conf_year: 2021
  url: "https://arxiv.org/pdf/2104.05321"
  abstract: "Fake tweets are observed to be ever-increasing, demanding immediate countermeasures to combat their spread. During COVID-19, tweets with misinformation should be flagged and neutralized in their early stages to mitigate the damages. Most of the existing methods for early detection of fake news assume to have enough propagation information for large labeled tweets -- which may not be an ideal setting for cases like COVID-19 where both aspects are largely absent. In this work, we present ENDEMIC, a novel early detection model which leverages exogenous and endogenous signals related to tweets, while learning on limited labeled data. We first develop a novel dataset, called CTF for early COVID-19 Twitter fake news, with additional behavioral test sets to validate early detection. We build a heterogeneous graph with follower-followee, user-tweet, and tweet-retweet connections and train a graph embedding model to aggregate propagation information. Graph embeddings and contextual features constitute endogenous, while time-relative web-scraped information constitutes exogenous signals. ENDEMIC is trained in a semi-supervised fashion, overcoming the challenge of limited labeled data. We propose a co-attention mechanism to fuse signal representations optimally. Experimental results on ECTF, PolitiFact, and GossipCop show that ENDEMIC is highly reliable in detecting early fake tweets, outperforming nine state-of-the-art methods significantly."