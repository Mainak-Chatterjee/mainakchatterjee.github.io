
<!doctype html>
<html lang="en">
<head>
<title>Measures of Information Reflect Memorization Patterns</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="description" content="Measures of Information Reflect Memorization Patterns" />
<meta property="og:title" content="Measures of Information Reflect Memorization Patterns" />
<meta property="og:url" content="https://rachitbansal.github.io/resources/information-measures" />
<!-- <meta property="og:url" content="https://rome.baulab.info/" />
<meta property="og:image" content="https://rome.baulab.info/images/ct-thumb.gif" /> -->
<meta property="og:description" content="Evaluating model generalization using intrinsic, information-theoretic metrics." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="twitter:title" content="Measures of Information Reflect Memorization Patterns" />
<meta name="twitter:description" content="Cracking open the black box of huge autoregressive transformer neural network language models." />
<meta name="twitter:image" href="/img/websites/information-measures-paper-thumb.png" />
<!-- <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest"> -->

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

<style>
.relatedthumb {
  float:left; width: 200px; margin: 3px 10px 7px 0;
}
.relatedblock {
  clear: both;
  display: inline-block;
}
.bold-sc {
  font-variant: small-caps;
  font-weight: bold;
}
.cite, .citegroup {
  margin-bottom: 8px;
}
:target {
  background-color: yellow;
}
</style>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
</script>

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">LMeasures of Information</nobr>
 <nobr class="widenobr">Reflect Memorization Patterns</nobr>
 </h1>
<address>
  <nobr><a href="https://rachitbansal.github.io/" target="_blank"
  >Rachit Bansal</a><sup>1</sup>,</nobr>
  <nobr><a href="https://baulab.info/" target="_blank"
  >Danish Pruthi</a><sup>2</sup>,</nobr>
  <nobr><a href="https://www.cs.technion.ac.il/~belinkov/" target="_blank"
  >Yonatan Belinkov</a><sup>3</sup></nobr>
 <br>
  <nobr><sup>1</sup><a href="https://www.csail.mit.edu/" target="_blank"
  >DTU (Now at Google AI)</a>,</nobr>
  <nobr><sup>2</sup><a href="https://khoury.northeastern.edu/" target="_blank"
  >Amazon AWS (Work done while at CMU LTI)</a>,</nobr>
  <nobr><sup>3</sup><a href="https://www.cs.technion.ac.il/" target="_blank"
  >Technion - IIT</a></nobr>;
</address>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">

<!-- <div class="row justify-content-center" style="margin-bottom: 20px">
<p class="text-center">
<a href="https://memit.baulab.info/"
   >Update!  See our <b>MEMIT</b> paper on scaling to thousands of facts:<br>
           Mass Editing Memory in a Transformer</a>
</p>
</div> -->

<div class="row justify-content-center text-center">
<p>
<a href="https://arxiv.org/pdf/2202.05262.pdf" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/paper-thumb.png" style="border:1px solid" alt="ArXiv Preprint thumbnail" data-nothumb=""><br>ArXiv<br>Preprint</a>
<a href="https://github.com/kmeng01/rome" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/code-thumb.png" style="border:1px solid" alt="Github code thumbnail" data-nothumb=""><br>Source Code<br>Github</a>
<a href="/data" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/data-thumb.png" style="border:1px solid" data-nothumb="" alt="Data directory thumbnail"><br>Dataset<br>and Models</a>
<a href="https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb" class="d-inline-block p-3 align-bottom" target="_blank"><img height="78" width="136" src="images/colab-thumb.png" style="border:1px solid" alt="Colab demo thumbnail" data-nothumb=""><br>Demo Colab:<br> Model Editing</a>
<a href="https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb" class="d-inline-block p-3 align-bottom" target="_blank"><img height="78" width="136" src="images/colab-thumb.png" style="border:1px solid" alt="Colab demo thumbnail" data-nothumb=""><br>Demo Colab:<br> Causal Tracing</a>
</p>

<div class="card" style="max-width: 1020px;">
<div class="card-block">
<!--
<h3>Can Facts be Located in a Large Neural Network?</h3>
<p>
Can knowledge be located within particular neurons
within a huge deep network?  We put GPT-style autoregressive
transformer language models under the microscope, and we find evidence
for a computational knowledge center in the network. This insight leads
to a state-of-the art method for editing a GPT model's factual knowledge.
</p>
-->
<h3>Where are the Facts Inside a Language Model?</h3>
<p>
<b>Knowing</b> differs from <b>saying</b>: uttering words by rote is
different from knowing a fact, because <em>knowledge of a fact generalizes
across contexts</em>. In this project, we show that factual knowledge
within GPT <b>also corresponds to a localized computation that can be
directly edited</b>. For example, we can make a small change to a small
set of the weights of GPT-J to teach it the counterfactual "Eiffel Tower
is located in the city of Rome." Rather than merely regurgitating the new
sentence, it will generalize that specific counterfactual knowledge and
apply it in very different linguistic contexts.
</p>
<img src="images/small-intro-animation.gif" class="medfig" alt="Diagram of a GPT transformer prediction">
</div><!--card-block-->
</div><!--card-->

</div><!--row-->

<div class="row">
<div class="col">
<h2>Why Locate Facts?</h2>

<!--
<p>Large language models such as GPT-2 and GPT-3 learn to predict many
factual statements about the world: for example, when completing the
sentence "Eiffel Tower is in the city of," large models will answer "Paris."
</p>
-->

<p>We are interested <b>how</b> and <b>where</b> a model stores its factual
associations, for two reasons:
</p>
<ol>
  <li><b>To understand huge opaque neural networks.</b> 
    The internal computations of large language models are obscure.
    Clarifying the processing of facts is one step in understanding
    massive transformer networks.
  <li><b>Fixing mistakes.</b> Models are often incorrect, biased,
    or private, and we would like to develop methods that will enable
    debugging and fixing of specific factual errors.
</ol>

<p>The facts we study take the form of knowledge tuples <b>t = (s, r, o)</b>, where
<b>s</b> and <b>o</b> are subject and object entities, respectively, and <b>r</b> is
the relation connecting the two. For example, <b>(s = <em>Megan Rapinoe</em>, r =
  <em>plays sport professionally</em>, o = <em>soccer</em>)</b> indicates that
Rapinoe plays soccer for a living. Each variable represents an 
entity or relation that can be found in a knowledge graph,
and that can be written as a natural language string.</p>

<p>To query GPT for knowledge of a fact, we
express <b>(s, r)</b> as a text prompt (by expanding a template from
the <span class="bold-sc">CounterFact</span> data set), and check whether
the generated continuation matches <b>o</b>.
</p>

<h2>What Did We Find?</h2>

<p>In GPT-style transformer models, we discovered two things:
</p>
<p><b>1. Factual associations can be localized</b> along three dimensions,
to (1) MLP module parameters (2) at a range of middle layers and
(3) specifically during processing of the last token
of the subject.
</p>
<p class="text-center">
<img id="shaq" src="images/teaser-shaq-crop.svg" class="smallfig" alt="A causal trace of a factual statement in GPT">
</p>
<p>
The causal trace above reveals a small number of states that
contain information that can flip the model from one factual prediction to
another.  Our studies use such causal traces and find evidence that knowledge
retrieval occurs in MLP modules at the early site (at (a) in the figure); then
attention mechanisms at the late site (at (b) in the figure) bring the
information to the end of the computation where the specific word can
be predicted.
</p>

<p style="padding-top: 10px"><b>2. Individual factual associations can be changed</b>
by making small
rank-one changes in a single MLP module.  We can distinguish between
changes in <em>knowledge</em> versus superficial changes in language by
measuring generalization to other wordings of the same fact.
</p>
<p class="text-center">
<img src="images/eiftower-crop.svg" class="smallfig" alt="An example of editing a fact in GPT using the ROME method.">
</p>
<p>
The example above shows that changing the model's processing of a single
statement about the Eiffel Tower, if done by changing selected parameters
in the right way, will result in expressing a change in knowledge in a
variety of nontrivial contexts.
</p>
<p>
At (a) in in the figure, a single direct statement of a counterfactual
is posed, and it is used to compute a rank-one parameter change in a single
MLP module.  Despite the simplicity of the change, results shown at (b) show
that for a more complex prompt about travel from Berlin, the model treats
the Eiffel tower as if it is in Rome; similarly in (c) when asked about nearby
sites, the model suggests places in Rome before explicitly mentioning Rome.
Changes in predictions in such different contexts is evidence that
change <em>generalizes</em>: the model has not merely learned to parrot
the exact sequence of words in the counterfactual, but it also applies
the new knowledge in sentences that are very different from the original
example.
</p>


<h2>How to Locate Factual Retrieval</h2>

<p>To identify decisive computations, we introduce a method called
<em>Causal Tracing</em>.
By isolating the causal effect of individual states within the
network while processing a factual statement, we can trace the path
followed by information through the network.
<p>

<p class="text-center">
<img src="images/small-ct-animation.gif" class="smallfig" alt="An animation demonstrating the Causal Tracing method.">
</p>

<p>Causal traces work by running a network multiple times,
introducing corruptions to frustrate the computation, and then
restoring individual states in order to identify the information
that restores the results.  Tracing can be used to test any
individual state or combinations of states.  We use carefully-designed
traces to identify a specific small set of MLP module computations
that mediate retrieval of factual associations.
</p>

<p>
Then we check this finding by asking: can the MLP module computations
be altered to edit a model's belief in a specific fact?
</p>

<h2>How to Edit Factual Storage</h2>

<p>To modify individual facts within a GPT model, we introduce a method
called <em>ROME</em>, or Rank-One Model Editing.
It treats an MLP module as a simple key-value store: for example, if
the key encodes a subject and the value encodes knowledge about the
subject, then the MLP can recall the association by retrieving the value
corresponding to the key.  ROME uses a rank-one modification of the
MLP weights to directly write in a new key-value pair.

<p class="text-center">
<img src="images/uv-update-crop.svg" class="smallfig" alt="Diagram of an MLP module">
</p>

<p>The figure above illustrates a single MLP module within a transformer.
The D-dimensional vector at (b) acts as the key that represents a
subject to know about, and the H-dimensional output at (c) 
acts at the value that encodes learned properties about the subject.
ROME inserts new association by making a rank-one change to 
the matrix (d) that maps from keys to values.
</p>
<p>
Note that ROME assumes a linear view of memory within a neural
network rather than an individual-neuron view.  This linear perspective
sees individual memories as rank-one slices of parameter space.  Experiments
confirm this view: when we do a rank-one update to an MLP module in the
computational center identified by causal tracing, we find that associations of
individual facts can be updated in a way that is both specific and generalized.
</p>

<h2>How to Distinguish Knowing a Fact from Saying a Fact</h2>

<b>Knowing</b> differs from <b>saying</b>.  
A variety of fine-tuning methods can cause a language model to
parrot a specific new sentence, but training a model to adjust its
knowledge of a fact is different from merely teaching it to regurgitate
a particular sequence of words.
</p>
<p>
We can tell the difference between knowing and saying
by measuring two hallmarks of knowledge: specificity and generalization.
</p>

<ol>
  <li><b>Specificity</b> means that when your knowledge of a fact changes,
    it doesn't change other facts.  For example, after learning that
    the Eiffel Tower is in Rome, you shouldn't also think that every
    other tourist attraction is also in Rome.
  <li><b>Generalization</b> means that your knowledge of a fact is robust
    to changes in wording and context.  After learning the Eiffel Tower is
    in Rome, then you should also know that visiting it will require travel
    to Rome.
</ol>

<p>Our new dataset
<a href="/data" class="bold-sc" target="_blank">CounterFact</a>
includes thousands of counterfactuals along with text that allows
quantitative testing of specificity and generalization when learning
a counterfactual.
</p>

<p class="text-center">
 <img src="images/knowing-saying.svg" class="bigfig" alt="Quantitative results distinguishing knowing from saying.">
</p>

<p>Above are the results of an experiment that uses <span class="bold-sc">CounterFact</span>
to confirm the distinction between knowing and saying parameters in GPT-2 XL.
ROME, which edits the <a href="#shaq">early causal site (a)</a>, achieves excellent efficacy
(measured by performance on the counterfactual prompt itself), specificity (performance on
neighborhood subjects <i>not</i> supposed to change), and generalization (performance on
paraphrases). By contrast, if we modify the attention mechanism at the <a href="#shaq">later site (b)</a>,
the model achieves fair efficacy and specificity but completely fails to generalize.
</p>

</div>
</div>

<div class="row">
<div class="col">

<h2>Related Work</h2>

<p>Our work builds upon insights in other work that has examined
large transformer language models and large neural networks from
several other perspectives:

<h3>Transformer Mechanisms</h3>
<p class="citation"><a href="https://transformer-circuits.pub/2021/framework/index.html"><img src="images/elhage-2021.png" alt="elhage-2021">Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah. A Mathematical Framework for Transformer Circuits. Anthropic 2021.</a><br>
<b>Notes:</b> Analyzes internal mechanisms of transformer components, developing mathematical tools for understanding patterns of computations.  Observes information-copying behavior in self-attention and implicates it in the strong performance of transformers.
<p class="citation"><a href="https://aclanthology.org/2021.emnlp-main.446.pdf"><img src="images/geva-2021.png" alt="geva-2021">Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy. Transformer Feed-Forward Layers Are Key-Value Memories.  EMNLP 2021.</a><br>
<b>Notes:</b> Proposes the view that transformer MLP modules act as key-value memories akin to two-layer softmax-based memory data structures.  Analyzes the contribution of these modules to token representations at each layer.
<p class="citation"><a href="https://arxiv.org/abs/2101.04547"><img src="images/zhao-2021.png" alt="zhao-2021">Sumu Zhao, Dami&aacute;n Pascual, Gino Brunner, Roger Wattenhofer. Of Non-Linearity and Commutativity in BERT. IJCNN 2021.</a><br>
<b>Notes:</b> Conducts a number of experiments of the computations of
transformer models, including an experiment that shows that swapping adjacent layers
of a transformer has only minimal impact on its behavior.

<h3>Extracting Knowledge from LMs</h3>

<p class="citation"><a href="https://arxiv.org/pdf/1909.01066.pdf"><img src="images/petroni-2019.png" alt="petroni-2019">Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel. Language Models as Knowledge Bases? EMNLP-IJCNLP 2019.</a><br>
<b>Notes:</b>
Proposes using fill-in-the-blank prompts for extracting knowledge from large language models.

<p class="citation"><a href="https://arxiv.org/pdf/1911.12543.pdf"><img src="images/jiang-2020.png" alt="jiang-2020">Zhengbao Jiang, Frank F. Xu, Jun Araki, Graham Neubig. How Can We Know What Language Models Know? TACL 2020.</a><br>
<b>Notes:</b> Discusses various ways to diversify prompts to improve extraction of knowledge from language models.

<p class="citation"><a href="https://arxiv.org/pdf/2002.08910.pdf"><img src="images/roberts-2020.png" alt="roberts-2020">Adam Roberts, Colin Raffel, Noam Shazeer.  How Much Knowledge Can You Pack Into the Parameters of a Language Model?  EMNLP 2020.</a><br>
<b>Notes:</b> Proposes fine-tuning a pretrained transformer language model to expand its ability to answer factual questions without reliance on an external knowledge source.

<p class="citation"><a href="https://arxiv.org/pdf/2104.05240.pdf"><img src="images/zhong-2021.png" alt="zhong-2021">Zexuan Zhong, Dan Friedman, Danqi Chen.  Factual Probing Is [MASK]: Learning vs. Learning to Recall.  NAACL 2021.</a><br>
<b>Notes:</b> Examines the use of learned knowledge probes for extracting knowledge, and also notes the risks of hallucinating new knowledge rather than extracting knowledge when using this technique.

<p class="citation"><a href="https://arxiv.org/pdf/2102.01017.pdf"><img src="images/elazer-2021.png" alt="elazer-2021">Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Sch&uuml;tze, Yoav Goldberg.  Measuring and Improving Consistency in Pretrained Language Models.  TACL 2021.</a><br>
<b>Notes:</b> Examines consistent generalization of language models, i.e., whether they predict the same facts under paraphrases.  The fact that models are often inconsistent under paraphrases can be seen as evidence that they do not have generalizable knowledge of some facts.   We use their <em>ParaRel</em> data set as the basis for
<span class="bold-sc">CounterFact</span>.

<h3>Causal Effects inside NNs</h3>

<p class="citation"><a href="https://arxiv.org/pdf/1907.07165.pdf"><img src="images/goyal-2020.png" alt="goyal-2020">Yash Goyal, Amir Feder, Uri Shalit, Been Kim.  Explaining Classifiers with Causal Concept Effect (CaCE).  2019.</a><br>
<b>Notes:</b> From computer vision; observes that causal explanations can come to different conclusion from a correlative analysis, and proposes ways to construct counterfactual explanations in computer vision.

<p class="citation"><a href="https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf"><img src="images/vig-2020.png" alt="vig-2020">Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber.  Investigating Gender Bias in Language Models Using Causal Mediation Analysis.  NeurIPS 2020.</a><br>
<b>Notes:</b> Applies causal mediation analysis to identify decisive neurons and attention heads responsible for gender bias in large language models.  Identifies a small handful of decisive attention heads in this case.
</p>

<p class="citation"><a href="https://arxiv.org/pdf/2005.13407.pdf"><img src="images/feder-2021.png" alt="feder-2021">Amir Feder, Nadav Oved, Uri Shalit, Roi Reichart.  CausaLM: Causal Model Explanation Through Counterfactual Language Models.  CL 2021.</a><br>
<b>Notes:</b> Devises a framework for understanding the structure of a language model by constructing representation-based counterfactuals and testing the model's causal response to them. 

<p class="citation"><a href="https://arxiv.org/pdf/2006.00995.pdf"><img src="images/elazer-2021b.png" alt="elazer-2021">Yanai Elazar, Shauli Ravfogel, Alon Jacovi, Yoav Goldberg.  Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals.  TACL 2021.</a><br>
<b>Notes:</b> Proposes measuring the importance of specific information within a model by introducing a causal intervention to erase that information, then observing the causal effects.

<h3>Knowledge Editing</h3>

<p class="citation"><a href="https://arxiv.org/pdf/2012.00363.pdf"><img src="images/zhu-2020.png" alt="zhu-2021">Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, Sanjiv Kumar. Modifying Memories in Transformer Models.  2020.</a><br>
<b>Notes:</b> Finds that a simple constrained fine-tuning, in which weights are constrained to lie near their pretrained values, is very effective at modifying learned knowledge within a transformer.

<p class="citation"><a href="https://arxiv.org/pdf/2104.08696.pdf"><img src="images/dai-2021.png" alt="dai-2021">Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei.  Knowledge Neurons in Pretrained Transformers. 2021.</a><br>
<b>Notes:</b> Building upon Geva (2021), proposes that individual neurons within MLP layers encode individual facts.  Describes an attribution method to find the neurons for a fact, and and conducts experiments manipulating these neurons to edit stored facts.

<p class="citation"><a href="https://arxiv.org/pdf/2104.08164.pdf"><img src="images/decao-2021.png" alt="decao-2021">Nicola De Cao, Wilker Aziz, Ivan Titov. Editing Factual Knowledge in Language Models. EMNLP 2021.</a><br>
<b>Notes:</b> Develops a "KnowledgeEditor" (KE) hypernetwork to fine-tune a model to incorporate a new fact given by a textual description of the fact.  The hypernetwork is an RNN that processes the description as well as the gradients of a loss to propose a complex multilayer change in the network.

<p class="citation"><a href="https://arxiv.org/abs/2110.11309"><img src="images/mitchell-2021.png" alt="mitchell-2021">Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning.  Fast Model Editing at Scale.  ICLR 2022.</a><br>
<b>Notes:</b> Develops a hypernetwork (MEND) to fine-tune a model to change its predictions to match a single run of text.  The hypernetwork uses gradients within the network to infer a small rank-one update to the model; the method is shown to scale to very large transformers.

<h3>Model Editing in Computer Vision</h3>

<p>Model Editing methods that use little or no training data have also been studied in computer vision.

<p class="citation"><a href="https://arxiv.org/pdf/2007.15646.pdf"><img src="images/bau-2020-rewriting.png" alt="bau-2020">David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba. Rewriting a Deep Generative Model. ECCV 2020.</a><br>
<b>Notes:</b> Demonstrates direct editing of associative rules within layers of a generative adversarial network (GAN), allowing a user to alter the appearance of objects in a model without supplying any new training images.  In our current work, we adopt the rank-one memory editing framework and apply it to large language model transformers.

<p class="citation"><a href="https://arxiv.org/pdf/2108.02774.pdf"><img src="images/wang-2021-sketch.png" alt="wang-2021">Sheng-Yu Wang, David Bau, Jun-Yan Zhu. Sketch Your Own GAN. ICCV 2021.</a><br>
<b>Notes:</b> Develops a method for altering a model using only a small number of user-provided sketches and without any new training photos.  Addresses the challenge of having user guidance that is given by examples in a much simpler data domain than the output data.

<p class="citation"><a href="https://arxiv.org/pdf/2108.00946.pdf"><img src="images/gal-2021-nada.png" alt="gal-2021">Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, Gal Chechik, Daniel Cohen-Or. StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators.</a><br>
<b>Notes:</b> Introduces the use of text guidance to alter a generative model without providing any new training images. Alters stylegan parameters using a directional CLIP objective that guides modified-model images to have specific differences with original-model images, and selects specific layers to modify based on their effect on the objective.

<h2>How to Cite</h2>

<p>This work will be presented at NeurIPS 2022.  It can be cited as follows.
</p>

<div class="card">
<h3 class="card-header">bibliography</h3>
<div class="card-block">
<p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. "<em>Locating and Editing Factual Associations in GPT.</em>" Advances in Neural Information Processing Systems 35 (2022).
</p>
</div>
<h3 class="card-header">bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
@article{meng2022locating,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2022}
}
</pre>
</div>
</div>
</p>

</div>
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://baulab.info/">About the Bau Lab</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>

